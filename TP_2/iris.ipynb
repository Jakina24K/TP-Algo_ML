{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"Reg & KNN util import\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\"\"\"\n",
    "REPONSE 1 :\n",
    "Le dataset Iris, créé par le biologiste britannique Edgar Anderson en 1935, est un ensemble de données célèbre dans le domaine de l'apprentissage automatique et de la statistique. Il contient des mesures de 150 fleurs d'iris réparties en trois espèces différentes : Iris setosa, Iris versicolor et Iris virginica.\n",
    "\n",
    "Les variables du dataset sont :\n",
    "\n",
    "    Longueur du sépale (sepal length)\n",
    "    Largeur du sépale (sepal width)\n",
    "    Longueur du pétale (petal length)\n",
    "    Largeur du pétale (petal width)\n",
    "\n",
    "L'objectif principal de l'étude est de classer ces différentes espèces d'iris en fonction de leurs caractéristiques morphologiques. Ce dataset est souvent utilisé pour des tâches de classification et pour illustrer des techniques d'apprentissage supervisé.\n",
    "\n",
    "REPONSE 2 :\n",
    "Nous avons utilisé le **pairplot**, qui permet de visualiser les relations entre deux variables à la fois. Par exemple, il aide à identifier si la longueur du sépale et celle du pétale sont corrélées, tout en montrant si cette corrélation varie selon l'espèce.\n",
    "\n",
    "REPONSE 3 :\n",
    "Les pétales permettent de mieux distinguer les espèces que les sépales. **Setosa** est facile à identifier, mais **Versicolor** et **Virginica** sont plus difficiles à séparer. Cette visualisation montre que chaque variable est importante pour la classification \n",
    "et qu'un algorithme supervisé pourrait bien fonctionner.\n",
    "\n",
    "REPONSE 5 :\n",
    "KNN est plus performant sur le dataset Iris grâce au choix optimal de k. Cependant, il est sensible au bruit et aux données déséquilibrées. La Régression Logistique reste rapide et efficace pour les problèmes linéaires. Le choix du modèle dépend des contraintes, comme \n",
    "le temps de calcul ou la complexité des données.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Charger le dataset Iris\n",
    "iris = load_iris(as_frame=True)\n",
    "\n",
    "# Créer un DataFrame\n",
    "iris_df = iris.data\n",
    "iris_df.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']  # Renommer les colonnes\n",
    "iris_df['species'] = iris.target.map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})  # Ajouter les noms des espèces\n",
    "\n",
    "# Vérifier le DataFrame\n",
    "print(iris_df.head())\n",
    "\n",
    "# Visualisation avec pairplot\n",
    "sns.pairplot(iris_df, vars=['sepal_length', 'sepal_width', 'petal_length', 'petal_width'], hue=\"species\", diag_kind=\"hist\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "X = iris.data[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]  # Exclude 'species' column\n",
    "y = iris.target\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Standardiser les données (important pour KNN)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\"--------------------------------Régression--------------------------------- \"\n",
    "\n",
    "# Appliquer la Régression Logistique\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred_log = log_reg.predict(X_test)\n",
    "\n",
    "# Évaluer la Régression Logistique\n",
    "log_reg_accuracy = accuracy_score(y_test, y_pred_log)\n",
    "\n",
    "print(f\"Précision de la Régression Logistique : {log_reg_accuracy:.2f}\")\n",
    "\n",
    "\"-------------------------------------KNN------------------------------------- \"\n",
    "\n",
    "# Choisir le meilleur k pour KNN\n",
    "k_values = range(1, 21)\n",
    "accuracies = []\n",
    "\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred_knn = knn.predict(X_test)\n",
    "    accuracies.append(accuracy_score(y_test, y_pred_knn))\n",
    "\n",
    "# Trouver le meilleur k\n",
    "best_k = k_values[accuracies.index(max(accuracies))]\n",
    "print(f\"\\n\\n Meilleur k pour KNN : {best_k}\")\n",
    "\n",
    "# Visualiser les performances pour différents k\n",
    "plt.plot(k_values, accuracies, marker='o')\n",
    "plt.xlabel(\"Nombre de voisins (k)\")\n",
    "plt.ylabel(\"Précision\")\n",
    "plt.title(\"Choix du meilleur k pour KNN\")\n",
    "plt.show()\n",
    "\n",
    "# Appliquer KNN avec le meilleur k\n",
    "knn_best = KNeighborsClassifier(n_neighbors=best_k)\n",
    "knn_best.fit(X_train, y_train)\n",
    "y_pred_knn_best = knn_best.predict(X_test)\n",
    "\n",
    "# Évaluer KNN\n",
    "knn_accuracy = accuracy_score(y_test, y_pred_knn_best)\n",
    "print(f\"Précision de KNN avec k={best_k} : {knn_accuracy:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
